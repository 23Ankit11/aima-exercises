<!DOCTYPE html>
<html>
  <head>
    <title>Main –  – </title>

        <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <meta name="description" content="">
    <meta property="og:description" content="" />
    
    <meta name="author" content="" />

    
    <meta property="og:title" content="Main" />
    <meta property="twitter:title" content="Main" />
    

    <!--[if lt IE 9]>
      <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="alternate" type="application/rss+xml" title=" - " href="/feed.xml" />
<!--     <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
 -->
    <!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->
  </head>

  <body>
    <input type="checkbox" id="toggleheader">
    <div class="wrapper-masthead">
      <div class="container">
        <header class="masthead clearfix">
          <center>
            <h1>Aritificial Intelligence: A Modern Approach</h1>
            <h3>Stuart J. Russell and Peter Norvig</h3>
          </center>
        </header>
      </div>
    </div>
    <input type="checkbox" id="toggletoc">
    <div class="toc">
      <div>Table of Contents</div>
      <ul>
	<li>
		<span>Part &#x2160; Artificial Intelligence</span>
		<ol>
			<li><a href="/intro-exercises">1. Introduction</a></li>
			<li><a href="/agents-exercises">2. Intelligent Agent</a></li>
		</ol>
	</li>
	<li>
		<span>Part &#x2161; Problem-solving</span>
		<ol>
			<li><a href="/search-exercises">3. Solving Problems By Searching</a></li>
			<li><a href="/advanced-search-exercises">4. Beyond Classical Search</a></li>
			<li><a href="/game-playing-exercises">5. Adversarial Search</a></li>
			<li><a href="/csp-exercises">6. Constraint Satisfaction Problems</a></li>
		</ol>
	</li>
	<li>
		<span>Part &#x2162; Knowledge, reasoning, and planning</span>
		<ol>
			<li><a href="/knowledge-logic-exercises">7. Logical Agents</a></li>
			<li><a href="/fol-exercises">8. First Order Logic</a></li>
			<li><a href="/logical-inference-exercises">9. Inference In First Order Logic</a></li>
			<li><a href="/planning-exercises">10. Classical Planning</a></li>
			<li><a href="/advanced-planning-exercises">11. Planning And Acting In The Real World</a></li>
			<li><a href="/kr-exercises">12. Knowledge Representation</a></li>
		</ol>
	</li>
	<li>
		<span>Part &#x2163; Uncertain knowledge and reasoning</span>
		<ol>
			<li><a href="/probability-exercises">13. Quantifying Uncertainity</a></li>
			<li><a href="/bayes-nets-exercises">14. Probabilistic Reasoning</a></li>
			<li><a href="/dbn-exercises">15. Probabilistic Reasoning Over Time</a></li>
			<li><a href="/decision-theory-exercises">16. Making Simple Decisions</a></li>
			<li><a href="/complex-decisions-exercises">17. Making Complex Decision</a></li>
		</ol>
	</li>
	<li>
		<span>Part &#x2164; Learning</span>
		<ol>
			<li><a href="/concept-learning-exercises">18. Learning From Examples</a></li>
			<li><a href="/ilp-exercises">19. Knowledge In Learning</a></li>
			<li><a href="/bayesian-learning-exercises">20. Learning Probabilistic Models</a></li>
			<li><a href="/reinforcement-learning-exercises">21. Reinforcement Learning</a></li>
		</ol>
	</li>
</ul>
    </div>
    <div id="main" role="main" class="container">
      



<ul class="breadcrumb">
<label for="toggletoc" class="toc-icon">
  <span></span>
  <span></span>
  <span></span>
</label>

   
    <li><a class="breadcrumb-text" href="/">home</a> &nbsp; </li>
   

<label for="toggleheader" class="toggleheader" title="Toggle Header">
    &#9167;
</label>
</ul>

      <article class="post">

  <div class="entry">
    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
      inlineMath: [ ['$','$'] ],
      displayMath: [ ['$$','$$'] ],
      processEscapes: true,
    },
    "HTML-CSS": { 
      preferredFont: "TeX", 
      availableFonts: ["STIX","TeX"], 
      styles: {".MathJax": {}} 
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<h1 id="probabilistic-reasoning-over-time">15. Probabilistic Reasoning over Time</h1>

<p><strong>15.1</strong> [state-augmentation-exercise] Show that any second-order Markov
process can be rewritten as a first-order Markov process with an
augmented set of state variables. Can this always be done
<em>parsimoniously</em>, i.e., without increasing the number of
parameters needed to specify the transition model?</p>

<p><strong>15.2</strong> [markov-convergence-exercise] In this exercise, we examine what
happens to the probabilities in the umbrella world in the limit of long
time sequences.</p>

<ol>
  <li>
    <p>Suppose we observe an unending sequence of days on which the
umbrella appears. Show that, as the days go by, the probability of
rain on the current day increases monotonically toward a
fixed point. Calculate this fixed point.</p>
  </li>
  <li>
    <p>Now consider <em>forecasting</em> further and further into the
future, given just the first two umbrella observations. First,
compute the probability $P(r_{2+k}|u_1,u_2)$ for
$k1\ldots {20}$ and plot the results. You should see that
the probability converges towards a fixed point. Prove that the
exact value of this fixed point is 0.5.</p>
  </li>
</ol>

<p><strong>15.3</strong> [island-exercise] This exercise develops a space-efficient variant of
the forward–backward algorithm described in
Figure <a href="#/">forward-backward-algorithm</a> (page <a href="#/">forward-backward-algorithm</a>).
We wish to compute ${\textbf{P}}(\textbf{X}<em>k|\textbf{e}</em>{1:t})$ for
$k1,\ldots ,t$. This will be done with a divide-and-conquer
approach.</p>

<ol>
  <li>
    <p>Suppose, for simplicity, that $t$ is odd, and let the halfway point
be $h(t+1)/2$. Show that
${\textbf{P}}(\textbf{X}<em>k|\textbf{e}</em>{1:t})$ can be computed for
$k1,\ldots ,h$ given just the initial forward message
$\textbf{f}<em>{1:0}$, the backward message $\textbf{b}</em>{h+1:t}$, and the evidence
$\textbf{e}_{1:h}$.</p>
  </li>
  <li>
    <p>Show a similar result for the second half of the sequence.</p>
  </li>
  <li>
    <p>Given the results of (a) and (b), a recursive divide-and-conquer
algorithm can be constructed by first running forward along the
sequence and then backward from the end, storing just the required
messages at the middle and the ends. Then the algorithm is called on
each half. Write out the algorithm in detail.</p>
  </li>
  <li>
    <p>Compute the time and space complexity of the algorithm as a function
of $t$, the length of the sequence. How does this change if we
divide the input into more than two pieces?</p>
  </li>
</ol>

<p><strong>15.4</strong> [flawed-viterbi-exercise] On page <a href="#/">flawed-viterbi-page</a>, we outlined a flawed
procedure for finding the most likely state sequence, given an
observation sequence. The procedure involves finding the most likely
state at each time step, using smoothing, and returning the sequence
composed of these states. Show that, for some temporal probability
models and observation sequences, this procedure returns an impossible
state sequence (i.e., the posterior probability of the sequence is
zero).</p>

<p><strong>15.5</strong> [hmm-likelihood-exercise]
Equation (<a href="#/">matrix-filtering-equation</a>) describes the
filtering process for the matrix formulation of HMMs. Give a similar
equation for the calculation of likelihoods, which was described
generically in Equation (<a href="#/">forward-likelihood-equation</a>).</p>

<p><strong>15.6</strong> Consider the vacuum worlds of
Figure <a href="#/">vacuum-maze-ch4-figure</a> (perfect sensing) and
Figure <a href="#/">vacuum-maze-hmm2-figure</a> (noisy sensing). Suppose
that the robot receives an observation sequence such that, with perfect
sensing, there is exactly one possible location it could be in. Is this
location necessarily the most probable location under noisy sensing for
sufficiently small noise probability $\epsilon$? Prove your claim or
find a counterexample.</p>

<p><strong>15.7</strong> [hmm-robust-exercise] In
Section <a href="#/">hmm-localization-section</a>, the prior
distribution over locations is uniform and the transition model assumes
an equal probability of moving to any neighboring square. What if those
assumptions are wrong? Suppose that the initial location is actually
chosen uniformly from the northwest quadrant of the room and the action
actually tends to move southeast[hmm-robot-southeast-page]. Keeping
the HMM model fixed, explore the effect on localization and path
accuracy as the southeasterly tendency increases, for different values
of $\epsilon$.</p>

<p><strong>15.8</strong> [roomba-viterbi-exercise] Consider a version of the vacuum robot
(page <a href="#/">vacuum-maze-hmm2-figure</a>) that has the policy of going straight for as long
as it can; only when it encounters an obstacle does it change to a new
(randomly selected) heading. To model this robot, each state in the
model consists of a <em>(location, heading)</em> pair. Implement
this model and see how well the Viterbi algorithm can track a robot with
this model. The robot’s policy is more constrained than the random-walk
robot; does that mean that predictions of the most likely path are more
accurate?</p>

<p><strong>15.9</strong> We have described three policies for the vacuum robot: (1) a uniform
random walk, (2) a bias for wandering southeast, as described in
Exercise <a href="#/">hmm-robust-exercise</a>, and (3) the policy
described in Exercise <a href="#/">roomba-viterbi-exercise</a>. Suppose
an observer is given the observation sequence from a vacuum robot, but
is not sure which of the three policies the robot is following. What
approach should the observer use to find the most likely path, given the
observations? Implement the approach and test it. How much does the
localization accuracy suffer, compared to the case in which the observer
knows which policy the robot is following?</p>

<p><strong>15.10</strong> This exercise is concerned with filtering in an environment with no
landmarks. Consider a vacuum robot in an empty room, represented by an
$n \times m$ rectangular grid. The robot’s location is hidden; the only
evidence available to the observer is a noisy location sensor that gives
an approximation to the robot’s location. If the robot is at location
$(x, y)$ then with probability .1 the sensor gives the correct location,
with probability .05 each it reports one of the 8 locations immediately
surrounding $(x, y)$, with probability .025 each it reports one of the
16 locations that surround those 8, and with the remaining probability
of .1 it reports “no reading.” The robot’s policy is to pick a direction
and follow it with probability .8 on each step; the robot switches to a
randomly selected new heading with probability .2 (or with probability 1
if it encounters a wall). Implement this as an HMM and do filtering to
track the robot. How accurately can we track the robot’s path?</p>

<p><strong>15.11</strong> This exercise is concerned with filtering in an environment with no
landmarks. Consider a vacuum robot in an empty room, represented by an
$n \times m$ rectangular grid. The robot’s location is hidden; the only
evidence available to the observer is a noisy location sensor that gives
an approximation to the robot’s location. If the robot is at location
$(x, y)$ then with probability .1 the sensor gives the correct location,
with probability .05 each it reports one of the 8 locations immediately
surrounding $(x, y)$, with probability .025 each it reports one of the
16 locations that surround those 8, and with the remaining probability
of .1 it reports “no reading.” The robot’s policy is to pick a direction
and follow it with probability .7 on each step; the robot switches to a
randomly selected new heading with probability .3 (or with probability 1
if it encounters a wall). Implement this as an HMM and do filtering to
track the robot. How accurately can we track the robot’s path?</p>

<center>
<b id="switching-kf-figure">Figure [switching-kf-figure]</b> A Bayesian network representation of a switching Kalman filter. The switching variable \(S_t\) is a discrete state variable whose value determines
the transition model for the continuous state variables $\textbf{X}_t$.
For any discrete state *i*, the transition model
$\textbf{P}(\textbf{X}_{t+1}|\textbf{X}_t,S_t= i)$ is a linear Gaussian model, just as in a
regular Kalman filter. The transition model for the discrete state,
$\textbf{P}(S_{t+1}|S_t)$, can be thought of as a matrix, as in a hidden
Markov model.
</center>

<p><img src="http://nalinc.github.io/aima-exercises/Jupyter%20notebook/figures/switching-kf.svg" alt="switching-kf-figure" /></p>

<p><strong>15.12</strong> [switching-kf-exercise] Often, we wish to monitor a continuous-state
system whose behavior switches unpredictably among a set of $k$ distinct
“modes.” For example, an aircraft trying to evade a missile can execute
a series of distinct maneuvers that the missile may attempt to track. A
Bayesian network representation of such a <strong>switching Kalman
filter</strong> model is shown in
Figure <a href="#switching-kf-figure">switching-kf-figure</a>.</p>

<ol>
  <li>
    <p>Suppose that the discrete state $S_t$ has $k$ possible values and
that the prior continuous state estimate
${\textbf{P}}(\textbf{X}_0)$ is a multivariate
Gaussian distribution. Show that the prediction
${\textbf{P}}(\textbf{X}_1)$ is a <strong>mixture of
Gaussians</strong>—that is, a weighted sum of Gaussians such
that the weights sum to 1.</p>
  </li>
  <li>
    <p>Show that if the current continuous state estimate
${\textbf{P}}(\textbf{X}<em>t|\textbf{e}</em>{1:t})$ is a mixture of $m$ Gaussians,
then in the general case the updated state estimate
${\textbf{P}}(\textbf{X}<em>{t+1}|\textbf{e}</em>{1:t+1})$ will be a mixture of
$km$ Gaussians.</p>
  </li>
  <li>
    <p>What aspect of the temporal process do the weights in the Gaussian
mixture represent?</p>
  </li>
</ol>

<p>The results in (a) and (b) show that the representation of the posterior
grows without limit even for switching Kalman filters, which are among
the simplest hybrid dynamic models.</p>

<p><strong>15.13</strong> [kalman-update-exercise] Complete the missing step in the derivation
of Equation (<a href="#/">kalman-one-step-equation</a>) on
page <a href="#/">kalman-one-step-equation</a>, the first update step for the one-dimensional Kalman
filter.</p>

<p><strong>15.14</strong> [kalman-variance-exercise] Let us examine the behavior of the variance
update in Equation (<a href="#/">kalman-univariate-equation</a>)
(page <a href="#/">kalman-univariate-equation</a>).</p>

<ol>
  <li>
    <p>Plot the value of $\sigma_t^2$ as a function of $t$, given various
values for $\sigma_x^2$ and $\sigma_z^2$.</p>
  </li>
  <li>
    <p>Show that the update has a fixed point $\sigma^2$ such that
$\sigma_t^2 \rightarrow \sigma^2$ as $t \rightarrow \infty$, and
calculate the value of $\sigma^2$.</p>
  </li>
  <li>
    <p>Give a qualitative explanation for what happens as
$\sigma_x^2\rightarrow 0$ and as $\sigma_z^2\rightarrow 0$.</p>
  </li>
</ol>

<p><strong>15.15</strong> [sleep1-exercise] A professor wants to know if students are getting
enough sleep. Each day, the professor observes whether the students
sleep in class, and whether they have red eyes. The professor has the
following domain theory:</p>

<ul>
  <li>
    <p>The prior probability of getting enough sleep, with no observations,
is 0.7.</p>
  </li>
  <li>
    <p>The probability of getting enough sleep on night $t$ is 0.8 given
that the student got enough sleep the previous night, and 0.3
if not.</p>
  </li>
  <li>
    <p>The probability of having red eyes is 0.2 if the student got enough
sleep, and 0.7 if not.</p>
  </li>
  <li>
    <p>The probability of sleeping in class is 0.1 if the student got
enough sleep, and 0.3 if not.</p>
  </li>
</ul>

<p>Formulate this information as a dynamic Bayesian network that the
professor could use to filter or predict from a sequence of
observations. Then reformulate it as a hidden Markov model that has only
a single observation variable. Give the complete probability tables for
the model.</p>

<p><strong>15.16</strong> [sleep1-exercise] A professor wants to know if students are getting
enough sleep. Each day, the professor observes whether the students
sleep in class, and whether they have red eyes. The professor has the
following domain theory:</p>

<ul>
  <li>
    <p>The prior probability of getting enough sleep, with no observations,
is 0.6.</p>
  </li>
  <li>
    <p>The probability of getting enough sleep on night $t$ is 0.8 given
that the student got enough sleep the previous night, and 0.2
if not.</p>
  </li>
  <li>
    <p>The probability of having red eyes is 0.2 if the student got enough
sleep, and 0.7 if not.</p>
  </li>
  <li>
    <p>The probability of sleeping in class is 0.1 if the student got
enough sleep, and 0.3 if not.</p>
  </li>
</ul>

<p>Formulate this information as a dynamic Bayesian network that the
professor could use to filter or predict from a sequence of
observations. Then reformulate it as a hidden Markov model that has only
a single observation variable. Give the complete probability tables for
the model.</p>

<p><strong>15.17</strong> For the DBN specified in Exercise <a href="#/">sleep1-exercise</a> and
for the evidence values</p>

<p><script type="math/tex">\textbf{e}_1 = not\space red\space eyes,\space not\space sleeping\space in\space class</script>
<script type="math/tex">\textbf{e}_2 = red\space eyes,\space not\space sleeping\space in\space class</script>
<script type="math/tex">\textbf{e}_3 = red\space eyes,\space sleeping\space in\space class</script></p>

<p>perform the following computations:</p>

<ol>
  <li>
    <p>State estimation: Compute $P({EnoughSleep}<em>t | \textbf{e}</em>{1:t})$ for each
of $t = 1,2,3$.</p>
  </li>
  <li>
    <p>Smoothing: Compute $P({EnoughSleep}<em>t | \textbf{e}</em>{1:3})$ for each of
$t = 1,2,3$.</p>
  </li>
  <li>
    <p>Compare the filtered and smoothed probabilities for $t=1$ and $t=2$.</p>
  </li>
</ol>

<p><strong>15.18</strong> Suppose that a particular student shows up with red eyes and sleeps in
class every day. Given the model described in
Exercise <a href="#/">sleep1-exercise</a>, explain why the probability
that the student had enough sleep the previous night converges to a
fixed point rather than continuing to go down as we gather more days of
evidence. What is the fixed point? Answer this both numerically (by
computation) and analytically.</p>

<p><strong>15.19</strong> [battery-sequence-exercise] This exercise analyzes in more detail the
persistent-failure model for the battery sensor in
Figure <a href="#/">battery-persistence-figure</a>(a)
(page <a href="#/">battery-persistence-figure</a>).</p>

<ol>
  <li>
    <p>Figure <a href="#/">battery-persistence-figure</a>(b) stops at
$t{32}$. Describe qualitatively what should happen as
$t\to\infty$ if the sensor continues to read 0.</p>
  </li>
  <li>
    <p>Suppose that the external temperature affects the battery sensor in
such a way that transient failures become more likely as
temperature increases. Show how to augment the DBN structure in
Figure <a href="#/">battery-persistence-figure</a>(a), and explain
any required changes to the CPTs.</p>
  </li>
  <li>
    <p>Given the new network structure, can battery readings be used by the
robot to infer the current temperature?</p>
  </li>
</ol>

<p><strong>15.20</strong> [dbn-elimination-exercise] Consider applying the variable elimination
algorithm to the umbrella DBN unrolled for three slices, where the query
is ${\textbf{P}}(R_3|u_1,u_2,u_3)$. Show that the space
complexity of the algorithm—the size of the largest factor—is the same,
regardless of whether the rain variables are eliminated in forward or
backward order.</p>

  </div>

<!--   <div class="date">
    Written on 
  </div>
 -->
  

</article>

    </div>

    <div class="wrapper-footer">
      <div class="container">
        <footer class="footer">
<!--            Designed by <a href="http://nalinc.github.io" style="color:#2196F3">Nalin</a> &#9889; 
           Written in <a href="#/" style="color: #2a932a">Markdown</a> &#9889; 
           Powered by <a href="http://jekyllrb.com" style="color: #d73838">Jekyll</a>    -->
          <!-- 











 -->
        </footer>
      </div>
    </div>

    

  </body>
</html>
