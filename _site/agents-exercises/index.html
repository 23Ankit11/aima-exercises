<!DOCTYPE html>
<html>
  <head>
    <title>Main –  – </title>

        <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <meta name="description" content="">
    <meta property="og:description" content="" />
    
    <meta name="author" content="" />

    
    <meta property="og:title" content="Main" />
    <meta property="twitter:title" content="Main" />
    

    <!--[if lt IE 9]>
      <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <script src="https://code.jquery.com/jquery-3.3.1.js"></script>
    <script src="//www.gstatic.com/firebasejs/5.0.4/firebase.js"></script>
    <script type="text/javascript" src="//aima-exercises.firebaseapp.com/config.js"></script>
    <script src="//http://www.gstatic.com/firebasejs/5.0.4/firebase-firestore.js"></script>

    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="alternate" type="application/rss+xml" title=" - " href="/feed.xml" />
<!--     <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
 -->
    <!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->
  </head>

  <body>
    <input type="checkbox" id="toggleheader">
    <div class="wrapper-masthead">
      <div class="container">
        <header class="masthead clearfix">
          <center>
            <h1>Aritificial Intelligence: A Modern Approach</h1>
            <h3>Stuart J. Russell and Peter Norvig</h3>
          </center>
        </header>
      </div>
    </div>
    <input type="checkbox" id="toggletoc">
    <div class="toc">
      <div>Table of Contents</div>
      <ul>
	<li>
		<span>Part &#x2160; Artificial Intelligence</span>
		<ol>
			<li><a href="/intro-exercises">1. Introduction</a></li>
			<li><a href="/agents-exercises">2. Intelligent Agent</a></li>
		</ol>
	</li>
	<li>
		<span>Part &#x2161; Problem-solving</span>
		<ol>
			<li><a href="/search-exercises">3. Solving Problems By Searching</a></li>
			<li><a href="/advanced-search-exercises">4. Beyond Classical Search</a></li>
			<li><a href="/game-playing-exercises">5. Adversarial Search</a></li>
			<li><a href="/csp-exercises">6. Constraint Satisfaction Problems</a></li>
		</ol>
	</li>
	<li>
		<span>Part &#x2162; Knowledge, reasoning, and planning</span>
		<ol>
			<li><a href="/knowledge-logic-exercises">7. Logical Agents</a></li>
			<li><a href="/fol-exercises">8. First Order Logic</a></li>
			<li><a href="/logical-inference-exercises">9. Inference In First Order Logic</a></li>
			<li><a href="/planning-exercises">10. Classical Planning</a></li>
			<li><a href="/advanced-planning-exercises">11. Planning And Acting In The Real World</a></li>
			<li><a href="/kr-exercises">12. Knowledge Representation</a></li>
		</ol>
	</li>
	<li>
		<span>Part &#x2163; Uncertain knowledge and reasoning</span>
		<ol>
			<li><a href="/probability-exercises">13. Quantifying Uncertainity</a></li>
			<li><a href="/bayes-nets-exercises">14. Probabilistic Reasoning</a></li>
			<li><a href="/dbn-exercises">15. Probabilistic Reasoning Over Time</a></li>
			<li><a href="/decision-theory-exercises">16. Making Simple Decisions</a></li>
			<li><a href="/complex-decisions-exercises">17. Making Complex Decision</a></li>
		</ol>
	</li>
	<li>
		<span>Part &#x2164; Learning</span>
		<ol>
			<li><a href="/concept-learning-exercises">18. Learning From Examples</a></li>
			<li><a href="/ilp-exercises">19. Knowledge In Learning</a></li>
			<li><a href="/bayesian-learning-exercises">20. Learning Probabilistic Models</a></li>
			<li><a href="/reinforcement-learning-exercises">21. Reinforcement Learning</a></li>
		</ol>
	</li>
	<li>
		<span>Part &#x2165; Communicating, perceiving, and acting</span>
		<ol>
			<li><a href="/nlp-communicating-exercises">22. Natural Language Processing</a></li>
			<li><a href="/nlp-english-exercises">23. Natural Language For Communication</a></li>
			<li><a href="/perception-exercises">24. Perception</a></li>
			<li><a href="/robotics-exercises">25. Robotics</a></li>
		</ol>
	</li>
	<li>
		<span>Part &#x2166; Conclusions</span>
		<ol>
			<li><a href="/philosophy-exercises">26. Philosophical Foundations</a></li>
			<li><a href="/#/"> Future Exercises</a></li>
		</ol>
	</li>
</ul>
    </div>
    <div id="main" role="main" class="container">
      



<ul class="breadcrumb">
<label for="toggletoc" class="toc-icon">
  <span></span>
  <span></span>
  <span></span>
</label>

   
    <li><a class="breadcrumb-text" href="/">home</a> &nbsp; </li>
   

<label for="toggleheader" class="toggleheader" title="Toggle Header">
    &#9167;
</label>
</ul>

      <article class="post">

  <div class="entry">
    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
      inlineMath: [ ['$','$'] ],
      displayMath: [ ['$$','$$'] ],
      processEscapes: true,
    },
    "HTML-CSS": { 
      preferredFont: "TeX", 
      availableFonts: ["STIX","TeX"], 
      styles: {".MathJax": {}} 
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<h1 id="2-intelligent-agents">2. Intelligent Agents</h1>

<p><a href="2-1/">Exercise 2.1</a></p>

<p>Suppose that the performance measure is concerned with just the first
$T$ time steps of the environment and ignores everything thereafter.
Show that a rational agent’s action may depend not just on the state of
the environment but also on the time step it has reached.</p>

<p><a href="2-2/">Exercise 2.2 [vacuum-rationality-exercise]</a></p>

<p>Let us examine the rationality of various
vacuum-cleaner agent functions.</p>
<ol>
  <li>
    <p>Show that the simple vacuum-cleaner agent function described in
Figure <a href="#/">vacuum-agent-function-table</a> is indeed
rational under the assumptions listed on page <a href="#/">vacuum-rationality-page</a>.</p>
  </li>
  <li>
    <p>Describe a rational agent function for the case in which each
movement costs one point. Does the corresponding agent program
require internal state?</p>
  </li>
  <li>
    <p>Discuss possible agent designs for the cases in which clean squares
can become dirty and the geography of the environment is unknown.
Does it make sense for the agent to learn from its experience in
these cases? If so, what should it learn? If not, why not?</p>
  </li>
</ol>

<p><a href="2-3/">Exercise 2.3</a></p>

<p>Write an essay on the relationship between evolution and one or more of
autonomy, intelligence, and learning.</p>

<p><a href="2-4/">Exercise 2.4</a></p>

<p>For each of the following assertions, say whether it is true or false
and support your answer with examples or counterexamples where
appropriate.</p>

<ol>
  <li>
    <p>An agent that senses only partial information about the state cannot
be perfectly rational.</p>
  </li>
  <li>
    <p>There exist task environments in which no pure reflex agent can
behave rationally.</p>
  </li>
  <li>
    <p>There exists a task environment in which every agent is rational.</p>
  </li>
  <li>
    <p>The input to an agent program is the same as the input to the
agent function.</p>
  </li>
  <li>
    <p>Every agent function is implementable by some
program/machine combination.</p>
  </li>
  <li>
    <p>Suppose an agent selects its action uniformly at random from the set
of possible actions. There exists a deterministic task environment
in which this agent is rational.</p>
  </li>
  <li>
    <p>It is possible for a given agent to be perfectly rational in two
distinct task environments.</p>
  </li>
  <li>
    <p>Every agent is rational in an unobservable environment.</p>
  </li>
  <li>
    <p>A perfectly rational poker-playing agent never loses.</p>
  </li>
</ol>

<p><a href="2-4/">Exercise 2.4 [PEAS-exercise]</a></p>

<p>For each of the following activities, give a PEAS
description of the task environment and characterize it in terms of the
properties listed in Section <a href="#/">env-properties-subsection</a>.</p>

<ul>
  <li>
    <p>Playing soccer.</p>
  </li>
  <li>
    <p>Exploring the subsurface oceans of Titan.</p>
  </li>
  <li>
    <p>Shopping for used AI books on the Internet.</p>
  </li>
  <li>
    <p>Playing a tennis match.</p>
  </li>
  <li>
    <p>Practicing tennis against a wall.</p>
  </li>
  <li>
    <p>Performing a high jump.</p>
  </li>
  <li>
    <p>Knitting a sweater.</p>
  </li>
  <li>
    <p>Bidding on an item at an auction.</p>
  </li>
</ul>

<p><a href="2-5/">Exercise 2.5 [PEAS-exercise] </a></p>

<p>For each of the following activities, give a PEAS
description of the task environment and characterize it in terms of the
properties listed in Section <a href="#/">env-properties-subsection</a>.</p>

<ul>
  <li>
    <p>Performing a gymnastics floor routine.</p>
  </li>
  <li>
    <p>Exploring the subsurface oceans of Titan.</p>
  </li>
  <li>
    <p>Playing soccer.</p>
  </li>
  <li>
    <p>Shopping for used AI books on the Internet.</p>
  </li>
  <li>
    <p>Practicing tennis against a wall.</p>
  </li>
  <li>
    <p>Performing a high jump.</p>
  </li>
  <li>
    <p>Bidding on an item at an auction.</p>
  </li>
</ul>

<p><a href="2-6/">Exercise 2.6</a></p>

<p>Define in your own words the following terms: agent, agent function,
agent program, rationality, autonomy, reflex agent, model-based agent,
goal-based agent, utility-based agent, learning agent.</p>

<p><a href="2-7/">Exercise 2.7 [agent-fn-prog-exercise]</a></p>

<p>This exercise explores the differences between
agent functions and agent programs.</p>

<ol>
  <li>
    <p>Can there be more than one agent program that implements a given
agent function? Give an example, or show why one is not possible.</p>
  </li>
  <li>
    <p>Are there agent functions that cannot be implemented by any agent
program?</p>
  </li>
  <li>
    <p>Given a fixed machine architecture, does each agent program
implement exactly one agent function?</p>
  </li>
  <li>
    <p>Given an architecture with $n$ bits of storage, how many different
possible agent programs are there?</p>
  </li>
  <li>
    <p>Suppose we keep the agent program fixed but speed up the machine by
a factor of two. Does that change the agent function?</p>
  </li>
</ol>

<p><a href="2-8/">Exercise 2.8</a></p>

<p>Write pseudocode agent programs for the goal-based and utility-based
agents.</p>

<p><a href="2-9/">Exercise 2.9</a></p>

<p>Consider a simple thermostat that turns on a furnace when the
temperature is at least 3 degrees below the setting, and turns off a
furnace when the temperature is at least 3 degrees above the setting. Is
a thermostat an instance of a simple reflex agent, a model-based reflex
agent, or a goal-based agent?</p>

<hr />
<p>The following exercises all concern the implementation of environments
and agents for the vacuum-cleaner world.</p>

<p><a href="2-10/">Exercise 2.10 [vacuum-start-exercise]</a></p>

<p>Implement a performance-measuring environment
simulator for the vacuum-cleaner world depicted in
Figure <a href="#/">vacuum-world-figure</a> and specified on
page <a href="#/">vacuum-rationality-page</a>. Your implementation should be modular so that the
sensors, actuators, and environment characteristics (size, shape, dirt
placement, etc.) can be changed easily. (<em>Note:</em> for some
choices of programming language and operating system there are already
implementations in the online code repository.)</p>

<p><a href="2-11/">Exercise 2.11</a></p>

<p>Implement a simple reflex agent for the vacuum environment in
Exercise <a href="#/">vacuum-start-exercise</a>. Run the environment
with this agent for all possible initial dirt configurations and agent
locations. Record the performance score for each configuration and the
overall average score.</p>

<p><a href="2-12/">Exercise 2.12 [vacuum-motion-penalty-exercise]</a></p>

<p>Consider a modified version of the
vacuum environment in Exercise <a href="#/">vacuum-start-exercise</a>,
in which the agent is penalized one point for each movement.</p>

<ol>
  <li>
    <p>Can a simple reflex agent be perfectly rational for this
environment? Explain.</p>
  </li>
  <li>
    <p>What about a reflex agent with state? Design such an agent.</p>
  </li>
  <li>
    <p>How do your answers to <strong>1</strong> and <strong>2</strong>
change if the agent’s percepts give it the clean/dirty status of
every square in the environment?</p>
  </li>
</ol>

<p><a href="2-13/">Exercise 2.13 [vacuum-unknown-geog-exercise]</a></p>

<p>consider a modified version of the
vacuum environment in Exercise <a href="#/">vacuum-start-exercise</a>,
in which the geography of the environment—its extent, boundaries, and
obstacles—is unknown, as is the initial dirt configuration. (The agent
can go <em>Up</em> and <em>Down</em> as well as <em>Left</em> and <em>Right</em>.)</p>

<ol>
  <li>
    <p>Can a simple reflex agent be perfectly rational for this
environment? Explain.</p>
  </li>
  <li>
    <p>Can a simple reflex agent with a <em>randomized</em> agent
function outperform a simple reflex agent? Design such an agent and
measure its performance on several environments.</p>
  </li>
  <li>
    <p>Can you design an environment in which your randomized agent will
perform poorly? Show your results.</p>
  </li>
  <li>
    <p>Can a reflex agent with state outperform a simple reflex agent?
Design such an agent and measure its performance on several
environments. Can you design a rational agent of this type?</p>
  </li>
</ol>

<p><a href="2-14/">Exercise 2.14 [vacuum-bump-exercise]</a></p>

<p>Repeat Exercise <a href="2-13/">vacuum-unknown-geog-exercise</a> for the case in
which the location sensor is replaced with a “bump” sensor that detects
the agent’s attempts to move into an obstacle or to cross the boundaries
of the environment. Suppose the bump sensor stops working; how should
the agent behave?</p>

<p><a href="2-15/">Exercise 2.15 [vacuum-finish-exercise]</a></p>

<p>The vacuum environments in the preceding
exercises have all been deterministic. Discuss possible agent programs
for each of the following stochastic versions:</p>

<ol>
  <li>
    <p>Murphy’s law: twenty-five percent of the time, the <em>Suck</em> action
fails to clean the floor if it is dirty and deposits dirt onto the
floor if the floor is clean. How is your agent program affected if
the dirt sensor gives the wrong answer 10% of the time?</p>
  </li>
  <li>
    <p>Small children: At each time step, each clean square has a 10%
chance of becoming dirty. Can you come up with a rational agent
design for this case?</p>
  </li>
</ol>


  </div>

<!--   <div class="date">
    Written on 
  </div>
 -->
  

</article>

    </div>

    <div class="wrapper-footer">
      <div class="container">
        <footer class="footer">
<!--            Designed by <a href="http://nalinc.github.io" style="color:#2196F3">Nalin</a> &#9889; 
           Written in <a href="#/" style="color: #2a932a">Markdown</a> &#9889; 
           Powered by <a href="http://jekyllrb.com" style="color: #d73838">Jekyll</a>    -->
          <!-- 











 -->
        </footer>
      </div>
    </div>

    


    <script type="text/javascript">
      firestore =firebase.firestore();
      function rateExercise(e){
        console.log(e.target)
        chapterLabel = $(e.target).data("chapter")
        exerciseLabel = $(e.target).data("exercise")
        docRef = firestore.collection("rating").doc(chapterLabel)
        score = 0
        docRef.get().then(function(doc){
          if (doc && doc.exists){
            myData = doc.data()
            // score = myData
            console.log(myData)
            if(exerciseLabel in myData){
              myData[exerciseLabel] += 1
            }else{
              myData[exerciseLabel] = 1
            }
            // $(e.target).data("rating",score);
            docRef.set(myData).then(function(){
              console.log("status saved")
              console.log(myData[exerciseLabel])
              $(e.target).attr("data-rating",myData[exerciseLabel]);
            })
          }
          // else{
          //   myData = {
          //     "ex_1":0,
          //     "ex_2":0,
          //     "ex_3":0,
          //     "ex_4":0,
          //     "ex_5":0,
          //     "ex_6":0,
          //     "ex_7":0,
          //     "ex_8":0,
          //     "ex_9":0,
          //     "ex_10":0
          //   }
          //   docRef.set(myData).then(function(){
          //     console.log("status saved")
          //     console.log(myData[exerciseLabel])
          //     // $(e.target).attr("data-rating",myData[exerciseLabel]);
          //   })
          // }
        })
      }
      getRealTimeUpdates = function(){
        docRef = firestore.collection("rating").doc("intro-exercises");
        docRef.onSnapshot(function(doc){
          if (doc && doc.exists){
            myData = doc.data()
            for(key in myData){
              console.log(key)  
              // $(e.target).attr("data-rating",myData[exerciseLabel]);
            }
          }
        })
      }

      getRealTimeUpdates()
      $(document).on("click",".arrow-up", rateExercise)
    </script>
  </body>
</html>
